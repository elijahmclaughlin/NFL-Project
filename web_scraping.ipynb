{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a3356e-8cf6-4a19-80d8-ab60d1d9ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a range of the data to scrape and preparing for web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "95e1252c-b05e-446a-a0c9-d2fc256dec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2002,2023))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d6b01e83-6038-4d2c-bd17-0c3c541d7a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ba951358-90d8-4ddd-bd6e-2de917db6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6decc509-8b7b-46a5-ab96-f075a4457b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing driver - using MacOS, so I used Safari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c44c84ab-9075-4b52-add3-f2add9c49d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Safari()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b58280-0490-4246-b3d4-040471126361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automating Safari to scrape the data from Pro Football Focus\n",
    "# also bypassing javascript and loading the whole page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2ee28456-b99c-461f-8255-258e6fa0b0c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url_games = \"https://www.pro-football-reference.com/years/{}/games.htm\"\n",
    "\n",
    "for year in years:\n",
    "    url = url_games.format(year)\n",
    "    \n",
    "    driver.get(url)\n",
    "    driver.execute_script(\"window.scrollTo(1,100)\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    with open(\"games/{}.html\".format(year), \"w+\") as f:\n",
    "        f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b8c1f32b-521a-43d5-b31e-504693802087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "40bd604b-0c9a-4125-a830-93c85c9d96bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae11c3a-9e6e-47a6-aa21-e5f7d6fc8673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming .html into usable dataframes\n",
    "# parsing for usable data -- and deleting unwanted rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0cd9575d-bb59-45a8-af8c-ff8cd0f82432",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for year in years:\n",
    "    with open(\"games/{}.html\".format(year)) as f:\n",
    "            page = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    \n",
    "    for thead_row in soup.find_all('tr', class_=\"thead\"):\n",
    "        thead_row.decompose()\n",
    "        \n",
    "    games_table = soup.find(id=\"games\")\n",
    "    games = pd.read_html(str(games_table))[0]\n",
    "    games[\"Year\"] = year\n",
    "    dfs.append(games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c7233092-1a9d-4642-a12b-098b868aaedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "games = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2e99a4-8902-4cbc-aab4-97f6a6085966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved into library as a .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "36035e01-31ce-4514-9090-84fd7ea1343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "games.to_csv(\"games.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dcbedb-3a9e-433e-9ec4-cb894a5ab283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat steps for more detailed stats on NFL franchises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3ecc0c70-c1b5-49c1-a9f2-6443148adb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_teamoff = \"https://www.pro-football-reference.com/years/{}/#team_stats\"\n",
    "\n",
    "for year in years:\n",
    "    url = url_teamoff.format(year)\n",
    "    \n",
    "    driver.get(url)\n",
    "    driver.execute_script(\"window.scrollTo(1,100)\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    with open(\"statistics/{}.html\".format(year), \"w+\") as f:\n",
    "        f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ff293f47-b6e7-4e20-a07b-b8404a667ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for year in years:\n",
    "    with open(\"statistics/{}.html\".format(year)) as f:\n",
    "            page = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    \n",
    "    for thead_row in soup.find_all('tr', class_=\"thead\"):\n",
    "        thead_row.decompose()\n",
    "    \n",
    "    for row_class in soup.find_all('tr', class_=\"average_table no_ranker\"):\n",
    "        row_class.decompose()\n",
    "        \n",
    "    for row_class in soup.find_all('tr', class_=\"league_total\"):\n",
    "        row_class.decompose()\n",
    "        \n",
    "    for row_class in soup.find_all('tr', class_=\"average_line\"):\n",
    "        row_class.decompose()\n",
    "    \n",
    "    passing_table = soup.find(id=\"div_passing\")\n",
    "    passing = pd.read_html(str(passing_table))[0]\n",
    "    passing[\"Year\"] = year\n",
    "    dfs.append(passing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c35f35f4-93e7-4de5-95e4-3d7609e1db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "passing = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6af4d2bc-45e4-4fba-bb6e-53c1f2785142",
   "metadata": {},
   "outputs": [],
   "source": [
    "passing.to_csv(\"passing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2e7a8594-6ee1-46da-b85a-3d40a150a579",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for year in years:\n",
    "    with open(\"statistics/{}.html\".format(year)) as f:\n",
    "            page = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    \n",
    "    for thead_row in soup.find_all('tr', class_=\"thead\"):\n",
    "        thead_row.decompose()\n",
    "    \n",
    "    for row_class in soup.find_all('tr', class_=\"average_table no_ranker\"):\n",
    "        row_class.decompose()\n",
    "        \n",
    "    for row_class in soup.find_all('tr', class_=\"league_total\"):\n",
    "        row_class.decompose()\n",
    "        \n",
    "    for row_class in soup.find_all('tr', class_=\"average_line\"):\n",
    "        row_class.decompose()\n",
    "    \n",
    "    rushing_table = soup.find(id=\"div_rushing\")\n",
    "    rushing = pd.read_html(str(rushing_table))[0]\n",
    "    rushing[\"Year\"] = year\n",
    "    dfs.append(rushing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ac8030c2-b0c5-4d96-9a0f-68eb29b454ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rushing = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6886218c-7fa8-45a1-980d-7ee64ed8b0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rushing.to_csv(\"rushing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "447085b9-bc36-4235-8f22-286bc23f0362",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for year in years:\n",
    "    with open(\"statistics/{}.html\".format(year)) as f:\n",
    "            page = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    \n",
    "    for thead_row in soup.find_all('tr', class_=\"thead\"):\n",
    "        thead_row.decompose()\n",
    "    \n",
    "    for row_class in soup.find_all('tr', class_=\"average_table no_ranker\"):\n",
    "        row_class.decompose()\n",
    "        \n",
    "    for row_class in soup.find_all('tr', class_=\"league_total\"):\n",
    "        row_class.decompose()\n",
    "        \n",
    "    for row_class in soup.find_all('tr', class_=\"average_line\"):\n",
    "        row_class.decompose()\n",
    "    \n",
    "    scoring_table = soup.find(id=\"div_team_scoring\")\n",
    "    scoring = pd.read_html(str(scoring_table))[0]\n",
    "    scoring[\"Year\"] = year\n",
    "    dfs.append(scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9f67f506-740f-4641-965c-424ed17331b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d1c66308-0f8b-433b-8382-fe8279ce2759",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring.to_csv(\"scoring.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4c331312-85df-4b00-bb0c-37df9969d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for year in years:\n",
    "    with open(\"statistics/{}.html\".format(year)) as f:\n",
    "            page = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    \n",
    "    for thead_row in soup.find_all('tr', class_=\"thead\"):\n",
    "        thead_row.decompose()\n",
    "    \n",
    "    for row_class in soup.find_all('tr', class_=\"average_table no_ranker\"):\n",
    "        row_class.decompose()\n",
    "        \n",
    "    for row_class in soup.find_all('tr', class_=\"league_total\"):\n",
    "        row_class.decompose()\n",
    "        \n",
    "    for row_class in soup.find_all('tr', class_=\"average_line\"):\n",
    "        row_class.decompose()\n",
    "    \n",
    "    for row_class in soup.find_all('tr', class_=\"over_header\"):\n",
    "        row_class.decompose()\n",
    "    \n",
    "    conversions_table = soup.find(id=\"div_team_conversions\")\n",
    "    conversions = pd.read_html(str(conversions_table))[0]\n",
    "    conversions[\"Year\"] = year\n",
    "    dfs.append(conversions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "10ad9838-0596-402c-b3d2-d75960478281",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversions = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f75644f5-a04b-4975-8f73-c68fbc2b9153",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversions.to_csv(\"conversions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "da4f6c3d-b884-4b63-b926-8a13430e1133",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for year in years:\n",
    "    with open(\"statistics/{}.html\".format(year)) as f:\n",
    "            page = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    \n",
    "    for thead_row in soup.find_all('tr', class_=\"thead\"):\n",
    "        thead_row.decompose()\n",
    "    \n",
    "    for row_class in soup.find_all('tr', class_=\"average_table no_ranker\"):\n",
    "        row_class.decompose()\n",
    "        \n",
    "    for row_class in soup.find_all('tr', class_=\"league_total\"):\n",
    "        row_class.decompose()\n",
    "        \n",
    "    for row_class in soup.find_all('tr', class_=\"average_line\"):\n",
    "        row_class.decompose()\n",
    "    \n",
    "    for row_class in soup.find_all('tr', class_=\"over_header\"):\n",
    "        row_class.decompose()\n",
    "    \n",
    "    drives_table = soup.find(id=\"div_drives\")\n",
    "    drives = pd.read_html(str(drives_table))[0]\n",
    "    drives[\"Year\"] = year\n",
    "    dfs.append(drives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cdc1ff4c-c8d7-4ab5-8bd4-ba5bb9a71d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drives = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bb4e5d0f-66cf-4a46-b17a-ee8ddf437a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drives.to_csv(\"drives.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f400b3a0-4521-4c17-9dfb-4550b0129e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for year in years:\n",
    "    with open(\"statistics/{}.html\".format(year)) as f:\n",
    "            page = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    \n",
    "    for thead_row in soup.find_all('tr', class_=\"thead onecell\"):\n",
    "        thead_row.decompose()\n",
    "        \n",
    "    for row_class in soup.find_all('tr', class_=\"over_header\"):\n",
    "        row_class.decompose()\n",
    "    \n",
    "    team_table = soup.find(id=\"all_AFC\")\n",
    "    team = pd.read_html(str(team_table))[0]\n",
    "    team[\"Year\"] = year\n",
    "    dfs.append(team)\n",
    "    \n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    \n",
    "    for thead_row in soup.find_all('tr', class_=\"thead onecell\"):\n",
    "        thead_row.decompose()\n",
    "        \n",
    "    for row_class in soup.find_all('tr', class_=\"over_header\"):\n",
    "        row_class.decompose()\n",
    "    \n",
    "    team_table = soup.find(id=\"all_NFC\")\n",
    "    team = pd.read_html(str(team_table))[0]\n",
    "    team[\"Year\"] = year\n",
    "    dfs.append(team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d27c83ff-0d57-4d0c-b233-f0d16ebd33a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "team = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9f98e156-ffe7-4bf3-b3d8-e309492fd1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "team.to_csv(\"team.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed36ff-9558-4b08-bac1-d572bd95a463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
